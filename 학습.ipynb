{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.0.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 데이터 개수: 84364\n",
      "Validation 데이터 개수: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zqrc0\\anaconda3\\envs\\summary\\lib\\site-packages\\huggingface_hub\\file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "c:\\Users\\zqrc0\\anaconda3\\envs\\summary\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7018faad0ab746ff955762b82412e102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/14061 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zqrc0\\anaconda3\\envs\\summary\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "# KoBART 관련\n",
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast, AdamW, get_linear_schedule_with_warmup\n",
    "import evaluate\n",
    "\n",
    "# 추가\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 6\n",
    "\n",
    "# 1. 데이터 로드 함수\n",
    "def load_preprocessed_data(base_dir):\n",
    "    input_texts = []\n",
    "    target_summaries = []\n",
    "    \n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "                    passage = \" \".join([f\"{d['character']}: {d['dialogue']}\" for d in data[\"passage\"]])\n",
    "                    summary = data[\"summaries\"].get(\"Summary1\", \"\")\n",
    "                    \n",
    "                    if passage and summary:\n",
    "                        input_texts.append(passage)\n",
    "                        target_summaries.append(summary)\n",
    "    return input_texts, target_summaries\n",
    "\n",
    "# 2. 데이터 로드\n",
    "train_base_dir = \"./pre_data/Training/TL1\"\n",
    "val_base_dir = \"./pre_data/Validation/VL1\"\n",
    "\n",
    "train_input_texts, train_target_summaries = load_preprocessed_data(train_base_dir)\n",
    "val_input_texts, val_target_summaries = load_preprocessed_data(val_base_dir)\n",
    "\n",
    "# Train 데이터와 Validation 데이터의 개수 출력\n",
    "print(f\"Train 데이터 개수: {len(train_input_texts)}\")\n",
    "print(f\"Validation 데이터 개수: {len(val_input_texts)}\")\n",
    "\n",
    "# 4. 모델 및 토크나이저 로드 (KoBART)\n",
    "model_name = \"gogamza/kobart-base-v1\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# 5. GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 6. 데이터 전처리 함수\n",
    "def preprocess_for_model(input_texts, target_summaries, tokenizer, max_input_length=1024, max_target_length=128):\n",
    "    inputs = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        target_summaries,\n",
    "        max_length=max_target_length,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs, targets\n",
    "\n",
    "# 7. 토큰화 (Train, Validation 각각)\n",
    "train_inputs, train_targets = preprocess_for_model(train_input_texts, train_target_summaries, tokenizer)\n",
    "val_inputs, val_targets = preprocess_for_model(val_input_texts, val_target_summaries, tokenizer)\n",
    "\n",
    "# 8. Dataset 클래스 정의\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.inputs[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n",
    "            \"labels\": self.targets[\"input_ids\"][idx],\n",
    "        }\n",
    "\n",
    "# 9. Dataset 생성\n",
    "train_dataset = SummaryDataset(train_inputs, train_targets)\n",
    "val_dataset = SummaryDataset(val_inputs, val_targets)\n",
    "\n",
    "# 10. DataLoader 생성\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 11. 옵티마이저 및 스케줄러 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "warmup_steps = int(0.1 * num_training_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# 12. 평가용 메트릭 (ROUGE)\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# 13. 검증(Validation) 함수\n",
    "def validate(model, val_dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "            decoded_labels = [tokenizer.decode(l, skip_special_tokens=True) for l in labels]\n",
    "            \n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "    \n",
    "    results = rouge.compute(predictions=predictions, references=references)\n",
    "    model.train()\n",
    "    return results, predictions\n",
    "\n",
    "# ----------------------------\n",
    "# 14. 학습 루프 (Train + Validation)\n",
    "# ----------------------------\n",
    "best_rouge_score = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "scaler = GradScaler()\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # tqdm을 이용한 진행률 표시\n",
    "    # desc: 진행 표시줄 왼쪽에 표시될 문구\n",
    "    # total: (선택) 총 step 개수를 명시적으로 표시하고 싶다면 len(train_dataloader)를 전달 가능\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"\\n=== Epoch {epoch+1} Done ===\")\n",
    "    print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # --- 검증(Validation) ---\n",
    "    val_results, val_predictions = validate(model, val_dataloader, tokenizer, device)\n",
    "    print(f\"Validation ROUGE: {val_results}\")\n",
    "\n",
    "    rouge_l_score = val_results[\"rougeL\"]\n",
    "    if rouge_l_score > best_rouge_score:\n",
    "        best_rouge_score = rouge_l_score\n",
    "        best_epoch = epoch + 1\n",
    "        model.save_pretrained(f\"./summary_model_checkpoint\")\n",
    "        tokenizer.save_pretrained(f\"./summary_model_checkpoint\")\n",
    "        print(f\"** Best model saved at epoch {best_epoch} with ROUGE-L: {best_rouge_score:.4f}\")\n",
    "\n",
    "# 15. 최종 모델 저장\n",
    "model.save_pretrained(\"./summary_model_final\")\n",
    "tokenizer.save_pretrained(\"./summary_model_final\")\n",
    "\n",
    "print(\"학습 및 검증 완료!\")\n",
    "print(f\"Best epoch: {best_epoch}, Best ROUGE-L: {best_rouge_score:.4f}\")\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
